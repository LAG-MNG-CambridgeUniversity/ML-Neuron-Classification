{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from time import time\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting correct path \n",
    "cwd = os.getcwd() # Get current working directory\n",
    "root_folder = os.sep + \"ML-Neuron-Classification\"\n",
    "sys.path.insert(0, cwd[:(cwd.index(root_folder)+len(root_folder))] + os.sep)\n",
    "from utils.helper_functions import*\n",
    "from utils.unpacking_dat_files import*\n",
    "\n",
    "nmi = normalized_mutual_info_score\n",
    "ari = adjusted_rand_score\n",
    "date = datetime.date.today()\n",
    "\n",
    "#Upload data\n",
    "data_path = 'C:/Users/Marius/Documents/Datasets/.txt/ec014.42_794_796_798_spikes.txt'\n",
    "x = np.genfromtxt(data_path, usecols=list(range(0,256)), skip_header=1)\n",
    "parameters = np.genfromtxt(data_path, dtype=None, encoding='UTF-8', usecols=list(range(256,267)), skip_header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Excting traces: 13155\n",
      "#Inhibiting traces: 13155\n",
      "#Total traces: 26310\n",
      "Self-training with 26310 traces!\n",
      "[0. 1.]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#Include paramter information into main array\n",
    "classification = np.empty(shape=(x.shape[0],1))\n",
    "for i in range(0, x.shape[0]):\n",
    "  if parameters[i][8] != 0 and parameters[i][9] == 0: #excitatory\n",
    "    classification[i] = 0\n",
    "  elif parameters[i][8] == 0 and parameters[i][9] != 0: #inhibitory\n",
    "    classification[i] = 1\n",
    "  else:\n",
    "    classification[i] = 2 #neither\n",
    "  i=i+1\n",
    "x = np.append(x, classification, axis=1)\n",
    "x = x[x[:,256] != 2] #No neither\n",
    "\n",
    "excits = x[x[:,256] == 0]\n",
    "inhibs = x[x[:,256] == 1]\n",
    "print(\"#Excting traces:\", excits.shape[0])\n",
    "\n",
    "\n",
    "rnd = np.random.choice(inhibs.shape[0], excits.shape[0], replace=False) #As more inhibs, choose randomly from inhibibitory to get same number of both types\n",
    "inhibs = inhibs[rnd,:]\n",
    "print(\"#Inhibiting traces:\", inhibs.shape[0])\n",
    "x = np.concatenate([excits, inhibs], axis=0) #back together\n",
    "x = np.take(x,np.random.permutation(x.shape[0]),axis=0,out=x) #random shuffle\n",
    "print(\"#Total traces:\", x.shape[0])\n",
    "\n",
    "Y_train = x[:,256]\n",
    "X = x[:,:-1]\n",
    "\n",
    "X_train = normalization_train(X)\n",
    "\n",
    "print('Self-training with', X_train.shape[0], 'traces!')\n",
    "n_clusters = len(np.unique(Y_train))\n",
    "print(np.unique(Y_train))\n",
    "print(n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "dims = [X_train.shape[-1], 500, 500, 2000, 7]\n",
    "#init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n",
    "pretrain_optimizer = 'Adam'\n",
    "pretrain_epochs = 50\n",
    "batch_size = 64\n",
    "save_dir = 'C:/Users/Marius/Documents/GitHub/ML-Neuron-Classification/Unsupervised learning/Model weights'\n",
    "name_save_process = f'/{date}_DEC_{pretrain_epochs}epochs_pretrain_1.h5'\n",
    "name_save_final = f'/{date}_DEC_{pretrain_epochs}epochs_final_1.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26310 samples\n",
      "Epoch 1/50\n",
      "26310/26310 [==============================] - 15s 568us/sample - loss: 1.7863e-04\n",
      "Epoch 2/50\n",
      "26310/26310 [==============================] - 15s 555us/sample - loss: 1.2501e-04\n",
      "Epoch 3/50\n",
      "26310/26310 [==============================] - 15s 567us/sample - loss: 1.1417e-04\n",
      "Epoch 4/50\n",
      "26310/26310 [==============================] - 15s 574us/sample - loss: 1.1225e-04\n",
      "Epoch 5/50\n",
      "26310/26310 [==============================] - 15s 571us/sample - loss: 1.1131e-04\n",
      "Epoch 6/50\n",
      "26310/26310 [==============================] - 17s 640us/sample - loss: 1.1050e-04\n",
      "Epoch 7/50\n",
      "26310/26310 [==============================] - 15s 579us/sample - loss: 1.0980e-04\n",
      "Epoch 8/50\n",
      "26310/26310 [==============================] - 15s 570us/sample - loss: 1.0924e-04\n",
      "Epoch 9/50\n",
      "26310/26310 [==============================] - 15s 578us/sample - loss: 1.0869e-04\n",
      "Epoch 10/50\n",
      "26310/26310 [==============================] - 16s 596us/sample - loss: 1.0829e-04\n",
      "Epoch 11/50\n",
      "26310/26310 [==============================] - 16s 624us/sample - loss: 1.0798e-04\n",
      "Epoch 12/50\n",
      "26310/26310 [==============================] - 15s 568us/sample - loss: 1.0777e-04\n",
      "Epoch 13/50\n",
      "26310/26310 [==============================] - 15s 570us/sample - loss: 1.0712e-04\n",
      "Epoch 14/50\n",
      "26310/26310 [==============================] - 16s 589us/sample - loss: 1.0691e-04\n",
      "Epoch 15/50\n",
      "26310/26310 [==============================] - 16s 605us/sample - loss: 1.0647e-04\n",
      "Epoch 16/50\n",
      "26310/26310 [==============================] - 16s 590us/sample - loss: 1.0608e-04\n",
      "Epoch 17/50\n",
      "26310/26310 [==============================] - 15s 589us/sample - loss: 1.0593e-04\n",
      "Epoch 18/50\n",
      "26310/26310 [==============================] - 17s 657us/sample - loss: 1.0571e-04\n",
      "Epoch 19/50\n",
      "26310/26310 [==============================] - 16s 590us/sample - loss: 1.0537e-04\n",
      "Epoch 20/50\n",
      "26310/26310 [==============================] - 16s 594us/sample - loss: 1.0539e-04\n",
      "Epoch 21/50\n",
      "26310/26310 [==============================] - 16s 599us/sample - loss: 1.0487e-04\n",
      "Epoch 22/50\n",
      "26310/26310 [==============================] - 16s 597us/sample - loss: 1.0462e-04\n",
      "Epoch 23/50\n",
      "26310/26310 [==============================] - 17s 647us/sample - loss: 1.0433e-04\n",
      "Epoch 24/50\n",
      "26310/26310 [==============================] - 16s 610us/sample - loss: 1.0411e-04\n",
      "Epoch 25/50\n",
      "26310/26310 [==============================] - 16s 597us/sample - loss: 1.0389e-04\n",
      "Epoch 26/50\n",
      "26310/26310 [==============================] - 17s 628us/sample - loss: 1.0368e-04\n",
      "Epoch 27/50\n",
      "26310/26310 [==============================] - 16s 615us/sample - loss: 1.0339e-04\n",
      "Epoch 28/50\n",
      "26310/26310 [==============================] - 16s 605us/sample - loss: 1.0322e-04\n",
      "Epoch 29/50\n",
      "26310/26310 [==============================] - 16s 599us/sample - loss: 1.0290e-04\n",
      "Epoch 30/50\n",
      "26310/26310 [==============================] - 16s 600us/sample - loss: 1.0268e-04\n",
      "Epoch 31/50\n",
      "26310/26310 [==============================] - 16s 603us/sample - loss: 1.0241e-04\n",
      "Epoch 32/50\n",
      "26310/26310 [==============================] - 16s 612us/sample - loss: 1.0235e-04\n",
      "Epoch 33/50\n",
      "26310/26310 [==============================] - 16s 620us/sample - loss: 1.0193e-04\n",
      "Epoch 34/50\n",
      "26310/26310 [==============================] - 16s 592us/sample - loss: 1.0185e-04\n",
      "Epoch 35/50\n",
      "26310/26310 [==============================] - 16s 608us/sample - loss: 1.0150e-04\n",
      "Epoch 36/50\n",
      "26310/26310 [==============================] - 17s 628us/sample - loss: 1.0130e-04\n",
      "Epoch 37/50\n",
      "26310/26310 [==============================] - 16s 620us/sample - loss: 1.0116e-04\n",
      "Epoch 38/50\n",
      "26310/26310 [==============================] - 16s 611us/sample - loss: 1.0092e-04\n",
      "Epoch 39/50\n",
      "26310/26310 [==============================] - 17s 634us/sample - loss: 1.0069e-04\n",
      "Epoch 40/50\n",
      "26310/26310 [==============================] - 16s 614us/sample - loss: 1.0055e-04\n",
      "Epoch 41/50\n",
      "26310/26310 [==============================] - 16s 617us/sample - loss: 1.0046e-04\n",
      "Epoch 42/50\n",
      "26310/26310 [==============================] - 16s 597us/sample - loss: 9.9944e-05\n",
      "Epoch 43/50\n",
      "26310/26310 [==============================] - 16s 600us/sample - loss: 9.9749e-05\n",
      "Epoch 44/50\n",
      "26310/26310 [==============================] - 17s 630us/sample - loss: 9.9521e-05\n",
      "Epoch 45/50\n",
      "26310/26310 [==============================] - 16s 605us/sample - loss: 9.9316e-05\n",
      "Epoch 46/50\n",
      "26310/26310 [==============================] - 16s 623us/sample - loss: 9.9220e-05\n",
      "Epoch 47/50\n",
      "26310/26310 [==============================] - 17s 638us/sample - loss: 9.9114e-05\n",
      "Epoch 48/50\n",
      "26310/26310 [==============================] - 17s 648us/sample - loss: 9.8740e-05\n",
      "Epoch 49/50\n",
      "26310/26310 [==============================] - 17s 630us/sample - loss: 9.8550e-05\n",
      "Epoch 50/50\n",
      "26310/26310 [==============================] - 17s 634us/sample - loss: 9.8307e-05\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = autoencoder(dims, init=init)\n",
    "autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "autoencoder.fit(X_train, X_train, batch_size=batch_size, epochs=pretrain_epochs)\n",
    "autoencoder.save_weights(save_dir + name_save_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optinal: Load weights\n",
    "autoencoder.load_weights(save_dir + name_save_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Initialize Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9054732041049031\n"
     ]
    }
   ],
   "source": [
    "#Clustering\n",
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "model.compile(optimizer='Adam', loss='kld')\n",
    "\n",
    "#Initialize Clusters using K-means\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=50)\n",
    "y_pred = kmeans.fit_predict(encoder.predict(X_train))\n",
    "y_pred_last = np.copy(y_pred)\n",
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "print(acc(Y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters iterative process\n",
    "loss = 0\n",
    "index = 0\n",
    "maxiter = 8000\n",
    "update_interval = 140\n",
    "index_array = np.arange(X_train.shape[0])\n",
    "tol = 0.0001 # tolerance threshold to stop training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: acc = 0.90547, nmi = 0.61674, ari = 0.65762  ; loss= 0\n",
      "Iter 140: acc = 0.91323, nmi = 0.63425, ari = 0.68301  ; loss= 0.0\n",
      "Iter 280: acc = 0.91619, nmi = 0.63901, ari = 0.69285  ; loss= 2e-05\n",
      "Iter 420: acc = 0.91722, nmi = 0.64401, ari = 0.69627  ; loss= 0.00011\n",
      "Iter 560: acc = 0.91680, nmi = 0.64066, ari = 0.69488  ; loss= 0.00083\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-9bd8be697963>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mite\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mite\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mupdate_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# update the auxiliary target distribution p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m               total_epochs=1)\n\u001b[0m\u001b[0;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q = model.predict(X_train, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "        # evaluate the clustering performance\n",
    "        y_pred = q.argmax(1)\n",
    "        if Y_train is not None:\n",
    "            acc_var = np.round(acc(Y_train, y_pred), 5)\n",
    "            nmi_var = np.round(nmi(Y_train, y_pred), 5)\n",
    "            ari_var = np.round(ari(Y_train, y_pred), 5)\n",
    "            loss = np.round(loss, 5)\n",
    "            print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc_var, nmi_var, ari_var), ' ; loss=', loss)\n",
    "\n",
    "        # check stop criterion - model convergence\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        if ite > 0 and delta_label < tol:\n",
    "            print('delta_label ', delta_label, '< tol ', tol)\n",
    "            print('Reached tolerance threshold. Stopping training.')\n",
    "            break\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, X_train.shape[0])]\n",
    "    loss = model.train_on_batch(x=X_train[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= X_train.shape[0] else 0\n",
    "\n",
    "model.save_weights(save_dir + name_save_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means accuracy plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8979475484606614\n",
      "0.9072215887495249\n",
      "0.9127708095781072\n",
      "0.9047890535917902\n",
      "0.8976434815659445\n",
      "0.8669327251995439\n",
      "0.9155454199923984\n",
      "0.8988217407829723\n",
      "0.9253515773470163\n",
      "0.9130368681109844\n",
      "0.9144051691372101\n",
      "0.9242873432155074\n",
      "0.911250475104523\n",
      "0.9055492208285822\n",
      "0.8952489547700494\n",
      "0.865488407449639\n",
      "0.9093120486507031\n",
      "0.9052451539338654\n",
      "0.8836944127708096\n",
      "0.9136830102622577\n",
      "[0.89794755 0.90722159 0.91277081 0.90478905 0.89764348 0.86693273\n",
      " 0.91554542 0.89882174 0.92535158 0.91303687 0.91440517 0.92428734\n",
      " 0.91125048 0.90554922 0.89524895 0.86548841 0.90931205 0.90524515\n",
      " 0.88369441 0.91368301]\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "dims = [X_train.shape[-1], 500, 500, 2000, 7]\n",
    "k_means = []\n",
    "#init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n",
    "pretrain_optimizer = 'Adam'\n",
    "pretrain_epochs = 50\n",
    "batch_size = 64\n",
    "save_dir = 'C:/Users/Marius/Documents/GitHub/ML-Neuron-Classification/Unsupervised learning/Model weights'\n",
    "\n",
    "for i in range (20):\n",
    "    def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "\t    n_stacks = len(dims) - 1\n",
    "\t    # input\n",
    "\t    input_img = Input(shape=(dims[0],), name='input')\n",
    "\t    x = input_img\n",
    "\t    # internal layers in encoder\n",
    "\t    for i in range(n_stacks-1):\n",
    "\t        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "\t    # hidden layer\n",
    "\t    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
    "\n",
    "\t    x = encoded\n",
    "\t    # internal layers in decoder\n",
    "\t    for i in range(n_stacks-1, 0, -1):\n",
    "\t        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "\t    # output\n",
    "\t    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "\t    decoded = x\n",
    "\t    return Model(inputs=input_img, outputs=decoded, name='AE'), Model(inputs=input_img, outputs=encoded, name='encoder')\n",
    "    autoencoder, encoder = autoencoder(dims)\n",
    "    autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "    name_save_process = f'/2020-10-19_DEC_{pretrain_epochs}epochs_{i}_pretrain.h5'\n",
    "    name_save_final = f'/2020-10-19_DEC_{pretrain_epochs}epochs_{i}_final.h5'\n",
    "    autoencoder.load_weights(save_dir + name_save_process)\n",
    "    #Clustering\n",
    "    clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "    model = Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "    model.compile(optimizer='Adam', loss='kld')\n",
    "\n",
    "    #Initialize Clusters using K-means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50)\n",
    "    y_pred = kmeans.fit_predict(encoder.predict(X_train))\n",
    "    y_pred_last = np.copy(y_pred)\n",
    "    model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "    print(acc(Y_train, y_pred))\n",
    "    k_means = np.append(k_means, acc(Y_train, y_pred))\n",
    "print(k_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final accuracy plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_means: 0.8977955150133029\n",
      "Final: 0.90665\n",
      "K_means: 0.9062713797035348\n",
      "Final: 0.91558\n",
      "K_means: 0.9127708095781072\n",
      "Final: 0.91817\n",
      "K_means: 0.904142911440517\n",
      "Final: 0.91376\n",
      "K_means: 0.8978715317369821\n",
      "Final: 0.91399\n",
      "K_means: 0.8670087419232231\n",
      "Final: 0.88419\n",
      "K_means: 0.9151653363740023\n",
      "Final: 0.9203\n",
      "K_means: 0.8968453059673128\n",
      "Final: 0.90996\n",
      "K_means: 0.925389585708856\n",
      "Final: 0.93379\n",
      "K_means: 0.9127708095781072\n",
      "Final: 0.91995\n",
      "K_means: 0.9139110604332953\n",
      "Final: 0.91657\n",
      "K_means: 0.9239452679589509\n",
      "Final: 0.93273\n",
      "K_means: 0.9108703914861269\n",
      "Final: 0.91699\n",
      "K_means: 0.905283162295705\n",
      "Final: 0.91193\n",
      "K_means: 0.8953629798555682\n",
      "Final: 0.90711\n",
      "K_means: 0.865868491068035\n",
      "Final: 0.87989\n",
      "K_means: 0.9091600152033448\n",
      "Final: 0.91859\n",
      "K_means: 0.9044849866970733\n",
      "Final: 0.90832\n",
      "K_means: 0.8827061953629799\n",
      "Final: 0.89168\n",
      "K_means: 0.9131508931965032\n",
      "Final: 0.91688\n",
      "K_means: 0.8980995819080198\n",
      "Final: 0.90802\n",
      "[0.90665 0.91558 0.91817 0.91376 0.91399 0.88419 0.9203  0.90996 0.93379\n",
      " 0.91995 0.91657 0.93273 0.91699 0.91193 0.90711 0.87989 0.91859 0.90832\n",
      " 0.89168 0.91688 0.90802]\n",
      "[1.99999995e-05 5.99999985e-05 4.99999987e-05 3.99999990e-05\n",
      " 3.99999990e-05 2.99999992e-05 4.99999987e-05 2.99999992e-05\n",
      " 3.99999990e-05 2.99999992e-05 7.00000019e-05 4.99999987e-05\n",
      " 3.99999990e-05 1.10000001e-04 2.99999992e-05 2.99999992e-05\n",
      " 4.99999987e-05 4.99999987e-05 1.19999997e-04 3.99999990e-05\n",
      " 3.99999990e-05]\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "dims = [X_train.shape[-1], 500, 500, 2000, 7]\n",
    "accs = []\n",
    "losses = []\n",
    "k_means = []\n",
    "#init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n",
    "pretrain_optimizer = 'Adam'\n",
    "pretrain_epochs = 50\n",
    "batch_size = 64\n",
    "save_dir = 'C:/Users/Marius/Documents/GitHub/ML-Neuron-Classification/Unsupervised learning/Model weights'\n",
    "\n",
    "for i in range (21):\n",
    "    def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "\t    n_stacks = len(dims) - 1\n",
    "\t    # input\n",
    "\t    input_img = Input(shape=(dims[0],), name='input')\n",
    "\t    x = input_img\n",
    "\t    # internal layers in encoder\n",
    "\t    for i in range(n_stacks-1):\n",
    "\t        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "\t    # hidden layer\n",
    "\t    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
    "\n",
    "\t    x = encoded\n",
    "\t    # internal layers in decoder\n",
    "\t    for i in range(n_stacks-1, 0, -1):\n",
    "\t        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "\t    # output\n",
    "\t    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "\t    decoded = x\n",
    "\t    return Model(inputs=input_img, outputs=decoded, name='AE'), Model(inputs=input_img, outputs=encoded, name='encoder')\n",
    "    autoencoder, encoder = autoencoder(dims)\n",
    "    autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "    name_save_process = f'/2020-10-19_DEC_{pretrain_epochs}epochs_{i}_pretrain.h5'\n",
    "    name_save_final = f'/2020-10-19_DEC_{pretrain_epochs}epochs_{i}_final.h5'\n",
    "    autoencoder.load_weights(save_dir + name_save_process)\n",
    "    #Clustering\n",
    "    clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "    model = Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "    model.compile(optimizer='Adam', loss='kld')\n",
    "\n",
    "    #Initialize Clusters using K-means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50)\n",
    "    y_pred = kmeans.fit_predict(encoder.predict(X_train))\n",
    "    y_pred_last = np.copy(y_pred)\n",
    "    model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "    print('K_means:',acc(Y_train, y_pred))\n",
    "    k_means = np.append(k_means, acc(Y_train, y_pred))\n",
    "    #Parameters iterative process\n",
    "    loss = 0\n",
    "    index = 0\n",
    "    maxiter = 1500\n",
    "    index_array = np.arange(X_train.shape[0])\n",
    "    tol = 0.01 # tolerance threshold to stop training\n",
    "    q = model.predict(X_train, verbose=0)\n",
    "    p = target_distribution(q)\n",
    "    for ite in range(int(maxiter)):\n",
    "        idx = index_array[index * batch_size: min((index+1) * batch_size, X_train.shape[0])]\n",
    "        loss = model.train_on_batch(x=X_train[idx], y=p[idx])\n",
    "        index = index + 1 if (index + 1) * batch_size <= X_train.shape[0] else 0\n",
    "        \n",
    "    # evaluate the clustering performance\n",
    "    model.save_weights(save_dir + name_save_final)\n",
    "    q = model.predict(X_train, verbose=0)\n",
    "    y_pred = q.argmax(1)\n",
    "    acc_var = np.round(acc(Y_train, y_pred), 5)\n",
    "    loss = np.round(loss, 5)\n",
    "    print('Final:', acc_var)\n",
    "    accs = np.append(accs, acc_var)\n",
    "    losses = np.append(losses, loss)\n",
    "print(accs)\n",
    "print(losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(k_means)\n",
    "plt.show()\n",
    "plt.hist(accs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means: 0.902803569166169 +- 0.015242627684260265\n",
      "Final DEC: 0.9116690476190477 +- 0.012926043074706234\n"
     ]
    }
   ],
   "source": [
    "print('K-means:', np.mean(k_means), '+-', np.std(k_means))\n",
    "print('Final DEC:', np.mean(accs), '+-', np.std(accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
